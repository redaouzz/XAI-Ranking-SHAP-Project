{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# README â€“ Running this notebook\n",
        "\n",
        "This notebook uses **RankingSHAP** as proposed by *Maria Heuss*, in the MQ2008 dataset.\n",
        "We worked with her open-source code available on GitHub to reproduce and extend experiments.  \n",
        "For a deeper understanding of the concept, please refer to the associated report where the methodology and interpretations are discussed in detail.\n",
        "\n",
        "## Access the necessary files\n",
        "\n",
        "All files required to run this notebook (even the dataset) are available in the following Drive folder:  \n",
        "ðŸ”— [RankingSHAP files on Drive](https://drive.google.com/drive/folders/1uoNotLMHnT8adxjhaTfmI2SKUKNDrxZb?usp=drive_link)\n",
        "\n",
        "## How to run this notebook\n",
        "\n",
        "There are two ways to set up the required files:\n",
        "\n",
        "### 1 **Using Google Drive (recommended if you have a Drive account)**\n",
        "\n",
        "- Download the `RankingShap` folder from the link above and place it into your own Google Drive.\n",
        "- In the notebook, mount your Drive and navigate to the folder thanks to the two first celluls\n",
        "\n",
        "###  2 Using local upload (if you prefer working locally)\n",
        "\n",
        "- Download the folder from the Drive link above.\n",
        "\n",
        "- Compress it as `RankingShap.zip` (if not already).\n",
        "\n",
        "- Upload and unzip the folder in the notebook thanks to the third and fourth cellul\n"
      ],
      "metadata": {
        "id": "l9yM-lMFnXt2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "OOTccm4GR4du"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd drive/MyDrive/RankingShap"
      ],
      "metadata": {
        "id": "SImuTRnWjGKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "DruUnLcTXao4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q RankingShap.zip"
      ],
      "metadata": {
        "id": "kyRnZJ9yXhTj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This cell installs all the required Python packages listed in the requirements.txt file.\n",
        "\"\"\"\n",
        "\n",
        "!pip install -r requirements.txt\n"
      ],
      "metadata": {
        "id": "pN1Zvobrm1Zq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset exploration\n",
        "\n",
        "In these cells, we explore the distribution of the dataset.  \n",
        "We compute and visualize the number of documents per query in the MQ2008 test set.  \n",
        "A histogram shows the distribution, and summary statistics (total queries, average, min, max documents per query) are printed.\n"
      ],
      "metadata": {
        "id": "CiaEkAX4a9vt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This cell defines a function to compute the number of documents per query ID (qid)\n",
        "from a LETOR-style dataset file. It generates a histogram showing the distribution\n",
        "of document counts per query for the test set, and prints summary statistics.\n",
        "\"\"\"\n",
        "\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def count_docs_per_query(filepath):\n",
        "    qid_counter = Counter()\n",
        "    with open(filepath, \"r\") as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split()\n",
        "            if len(parts) < 2:\n",
        "                continue  # Skip empty or invalid lines\n",
        "            qid = int(parts[1].split(\":\")[1])  # Extract qid from the second field (format qid:XYZ)\n",
        "            qid_counter[qid] += 1  # Count occurrence of each qid\n",
        "    return qid_counter\n",
        "\n",
        "# Load the test set file\n",
        "file_test = \"data/MQ2008/Fold1/test_1.txt\"\n",
        "combined_counts = count_docs_per_query(file_test)\n",
        "\n",
        "# Get document counts per query for plotting\n",
        "doc_per_query = list(combined_counts.values())\n",
        "\n",
        "# Define histogram bins\n",
        "bins = [0, 5, 10, 15, 20, 30, 50, 100]\n",
        "\n",
        "# Plot histogram\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(doc_per_query, bins=bins, color='skyblue', edgecolor='black')\n",
        "plt.xlabel(\"Number of documents per query\")\n",
        "plt.ylabel(\"Number of queries\")\n",
        "plt.title(\"Distribution of documents per query in MQ2008 (test set)\")\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()\n",
        "\n",
        "# Print summary statistics\n",
        "print(f\"Total number of queries: {len(combined_counts)}\")\n",
        "print(f\"Average number of documents per query: {sum(doc_per_query) / len(doc_per_query):.2f}\")\n",
        "print(f\"Max documents in a query: {max(doc_per_query)}\")\n",
        "print(f\"Min documents in a query: {min(doc_per_query)}\")\n"
      ],
      "metadata": {
        "id": "-3dTn6OymTgF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Query selection for analysis\n",
        "\n",
        "In this study, we work on a subset of **50 queries** out of the original 136 queries in the MQ2008 test set.  \n",
        "\n",
        "The selected queries represent the original dataset distribution in terms of the number of documents per query:  \n",
        "- **10 rich queries** (with 30 or more documents)  \n",
        "- **20 medium queries** (with between 11 and 29 documents)  \n",
        "- **20 poor queries** (with fewer than 11 documents)  \n",
        "\n",
        "This sampling ensures a balanced and representative selection for further analysis.\n"
      ],
      "metadata": {
        "id": "2qonWyWAb52q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rich = [qid for qid, c in combined_counts.items() if c >= 30]\n",
        "medium = [qid for qid, c in combined_counts.items() if 11 <= c < 30]\n",
        "poor = [qid for qid, c in combined_counts.items() if c < 11]\n",
        "\n",
        "import random\n",
        "\n",
        "random.seed(42)  # reproductability\n",
        "\n",
        "selected_rich = random.sample(rich, min(10, len(rich)))\n",
        "selected_medium = random.sample(medium, min(20, len(medium)))\n",
        "selected_poor = random.sample(poor, min(20, len(poor)))\n",
        "\n",
        "selected_queries = selected_rich + selected_medium + selected_poor\n",
        "print(f\"Selected {len(selected_queries)} queries\")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "all_counts = [combined_counts[qid] for qid in selected_queries]\n",
        "plt.hist(all_counts, bins=10, color='skyblue', edgecolor='black')\n",
        "plt.xlabel(\"Number of documents per query (selected)\")\n",
        "plt.ylabel(\"Number of queries\")\n",
        "plt.title(\"Distribution of selected queries\")\n",
        "plt.show()\n",
        "\n",
        "print(f\"Rich: {len(selected_rich)} queries\")\n",
        "print(f\"Medium: {len(selected_medium)} queries\")\n",
        "print(f\"Poor: {len(selected_poor)} queries\")\n",
        "print(f\"Total: {len(selected_queries)} queries\")\n"
      ],
      "metadata": {
        "id": "MDQzDygEjylu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test set filtering\n",
        "\n",
        "We filtered the MQ2008 test set to create a new test file containing only the 50 selected queries.  \n",
        "These queries represent different levels of document richness to ensure a balanced and representative subset.  \n",
        "\n",
        "The steps performed were:\n",
        "- Lines corresponding to the selected query IDs were extracted from the original `test_1.txt` file.\n",
        "- The filtered data was saved as `test_selected50.txt`.\n",
        "- The file was then renamed to `test.txt` to be used in further analysis.\n",
        "\n",
        "This ensures that all subsequent processing is performed on the chosen subset of 50 queries.\n"
      ],
      "metadata": {
        "id": "03kHdbeoeylC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This cell defines a function to read a LETOR-style dataset file and extract features, labels, and query IDs.\n",
        "It loads the test set from 'test_1.txt', creates a DataFrame of qids, filters for the selected queries,\n",
        "and displays the number of documents per selected query in descending order.\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Function to read a LETOR-style file\n",
        "def read_letor_file(filepath):\n",
        "    X = []\n",
        "    y = []\n",
        "    qids = []\n",
        "\n",
        "    with open(filepath, \"r\") as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split()\n",
        "            if len(parts) < 2:\n",
        "                continue  # Skip empty or invalid lines\n",
        "            y_val = int(parts[0])\n",
        "            qid = int(parts[1].split(\":\")[1])\n",
        "            features = []\n",
        "            for item in parts[2:]:\n",
        "                if item.startswith(\"#\"):\n",
        "                    break  # Ignore comments\n",
        "                _, val = item.split(\":\")\n",
        "                features.append(float(val))\n",
        "            X.append(features)\n",
        "            y.append(y_val)\n",
        "            qids.append(qid)\n",
        "\n",
        "    return np.array(X), np.array(y), np.array(qids)\n",
        "\n",
        "# Load the test set\n",
        "X_test, y_test, qids_test = read_letor_file(\"data/MQ2008/Fold1/test_1.txt\")\n",
        "\n",
        "# Create a DataFrame for qids\n",
        "df_qid = pd.DataFrame({'qid': qids_test})\n",
        "\n",
        "# Filter for selected queries (assumes selected_queries already exists)\n",
        "df_selected = df_qid[df_qid['qid'].isin(selected_queries)]\n",
        "\n",
        "# Count documents per query and sort\n",
        "query_doc_counts_selected = df_selected.groupby('qid').size().sort_values(ascending=False)\n",
        "\n",
        "# Display results\n",
        "print(\"âœ… Selected queries and their document counts:\")\n",
        "print(query_doc_counts_selected.to_string())\n",
        "\n",
        "# Optionally extract sorted query IDs\n",
        "selected_query_ids = query_doc_counts_selected.index.tolist()\n",
        "print(\"\\nSelected query IDs (test set):\", selected_query_ids)\n"
      ],
      "metadata": {
        "id": "bZcDHy19npe_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This cell filters the test set file to include only the selected 50 queries,\n",
        "writes the filtered data to a new file, and renames that file as the main test file for further use.\n",
        "\"\"\"\n",
        "\n",
        "input_path = \"data/MQ2008/Fold1/test_1.txt\"\n",
        "output_path = \"data/MQ2008/Fold1/test_selected50.txt\"\n",
        "\n",
        "# Set of selected qids as strings\n",
        "selected_qids = {\n",
        "    '19353', '19782', '18230', '19851', '18511', '18490', '18525', '18826', '19960', '18844',\n",
        "    '19059', '18571', '19034', '18714', '18401', '19276', '19457', '18429', '19383', '19836',\n",
        "    '18577', '18378', '19913', '18386', '18599', '18996', '19449', '18468', '19396', '19503',\n",
        "    '19372', '19487', '19454', '19954', '19400', '19153', '19371', '19356', '19140', '19108',\n",
        "    '18342', '19003', '18767', '18686', '18552', '18531', '19370', '18437', '18489', '18889'\n",
        "}\n",
        "\n",
        "# Read the input file and write lines that match the selected qids to the output file\n",
        "with open(input_path, 'r') as infile, open(output_path, 'w') as outfile:\n",
        "    for line in infile:\n",
        "        qid_part = [p for p in line.split() if p.startswith(\"qid:\")][0]\n",
        "        qid = qid_part.split(\":\")[1]\n",
        "        if qid in selected_qids:\n",
        "            outfile.write(line)\n",
        "\n",
        "print(f\"âœ… New file generated: {output_path}\")\n",
        "\n",
        "import os\n",
        "\n",
        "# Rename the filtered file as the new test set\n",
        "os.rename(\"data/MQ2008/Fold1/test_selected50.txt\", \"data/MQ2008/Fold1/test.txt\")\n"
      ],
      "metadata": {
        "id": "9V8K8nckqEdH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model training and evaluation with RankingSHAP\n",
        "\n",
        "In these steps, we train a listwise ranking model on the MQ2008 dataset and apply RankingSHAP for explanation.\n",
        "\n",
        "- The `lime` package is installed to support model explainability.\n",
        "- A ranking model is trained using `train_model.py` with the listwise approach.\n",
        "- The RankingSHAP test script is made executable and run to generate explanations on the test set.\n",
        "\n",
        "**Warning:** Running these cells will take a very long time (estimated runtime: up to 32 hours).  \n",
        "Make sure you are prepared for extended computations before launching them.\n"
      ],
      "metadata": {
        "id": "MIVekz8wfcne"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lime\n"
      ],
      "metadata": {
        "id": "kOKULM4xdcE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python train_model.py --dataset MQ2008 --file_name test_model --model_type listwise\n"
      ],
      "metadata": {
        "id": "X0poVWPNdNmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod +x run_rankingshap_test.sh"
      ],
      "metadata": {
        "id": "0ghsZlkcq7w1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./run_rankingshap_test.sh"
      ],
      "metadata": {
        "id": "5kqfig12rX5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature attribution visualization\n",
        "\n",
        "These steps visualize the feature attributions computed by RankingSHAP:\n",
        "\n",
        "- Individual bar plots are generated for each query, showing the attribution values for all features.\n",
        "- A global bar plot is created by computing the mean attribution of each feature across all selected queries.  \n",
        "  This provides an overall view of feature importance within the dataset.\n"
      ],
      "metadata": {
        "id": "yVF2fbh0apT3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This cell loads the RankingSHAP output file and generates bar plots for each query.\n",
        "Each plot shows the feature attribution values computed by RankingSHAP for a given query.\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the RankingSHAP result file\n",
        "df = pd.read_csv(\"results/results_MQ2008/feature_attributes/rankingshap.csv\")\n",
        "\n",
        "# List of query IDs from the CSV columns\n",
        "query_ids = df.columns.tolist()\n",
        "query_ids.remove(\"feature_number\")  # Remove the feature number column\n",
        "\n",
        "# Generate a bar plot for each query\n",
        "for qid in query_ids:\n",
        "    df_sorted = df.sort_values(by=qid, ascending=False)\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.bar(df_sorted['feature_number'].astype(str), df_sorted[qid])\n",
        "    plt.title(f\"RankingSHAP: Attribution of features for the query {qid}\")\n",
        "    plt.xlabel(\"Feature number\")\n",
        "    plt.ylabel(\"Attribution value\")\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "qC6HKfWAPQ7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This cell loads the RankingSHAP output file and computes the mean attribution value\n",
        "for each feature across all selected queries. It generates a bar plot showing\n",
        "the global importance of features based on these mean values.\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the RankingSHAP attribution file\n",
        "df = pd.read_csv(\"results/results_MQ2008/feature_attributes/rankingshap.csv\")\n",
        "\n",
        "# Get the columns corresponding to query IDs\n",
        "query_cols = [col for col in df.columns if col != \"feature_number\"]\n",
        "\n",
        "# Compute the mean attribution value for each feature\n",
        "df['mean_attribution'] = df[query_cols].mean(axis=1)\n",
        "\n",
        "# Sort features by mean attribution in descending order\n",
        "df_sorted = df.sort_values(by='mean_attribution', ascending=False)\n",
        "\n",
        "# Plot the bar chart\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(df_sorted['feature_number'].astype(str), df_sorted['mean_attribution'])\n",
        "plt.title(\"RankingSHAP: Global Attribution of features (average on 50 queries)\")\n",
        "plt.xlabel(\"Feature number\")\n",
        "plt.ylabel(\"Mean attribution value\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "JyPMBbqMP_Na"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Charger ton fichier RankingSHAP\n",
        "df = pd.read_csv(\"results/results_MQ2008/feature_attributes/rankingshap.csv\")\n",
        "\n",
        "# Mapping of features\n",
        "feature_map = {\n",
        "    1: \"TF-IDF body\",\n",
        "    2: \"TF-IDF anchor\",\n",
        "    3: \"TF-IDF title\",\n",
        "    4: \"TF-IDF URL\",\n",
        "    5: \"BM25 body\",\n",
        "    6: \"BM25 anchor\",\n",
        "    7: \"BM25 title\",\n",
        "    8: \"BM25 URL\",\n",
        "    9: \"LMIR.ABS body\",\n",
        "    10: \"LMIR.ABS anchor\",\n",
        "    11: \"LMIR.ABS title\",\n",
        "    12: \"LMIR.ABS URL\",\n",
        "    13: \"LMIR.DIR body\",\n",
        "    14: \"LMIR.DIR anchor\",\n",
        "    15: \"LMIR.DIR title\",\n",
        "    16: \"LMIR.DIR URL\",\n",
        "    17: \"LMIR.JM body\",\n",
        "    18: \"LMIR.JM anchor\",\n",
        "    19: \"LMIR.JM title\",\n",
        "    20: \"LMIR.JM URL\",\n",
        "    21: \"PageRank\",\n",
        "    22: \"Inlink number\",\n",
        "    23: \"Outlink number\",\n",
        "    24: \"URL slashes count\",\n",
        "    25: \"URL length\",\n",
        "    26: \"Child pages count\",\n",
        "    27: \"Query IDF\",\n",
        "    28: \"Sum TF body\",\n",
        "    29: \"Sum TF anchor\",\n",
        "    30: \"Sum TF title\",\n",
        "    31: \"Sum TF URL\",\n",
        "    32: \"Min TF body\",\n",
        "    33: \"Min TF anchor\",\n",
        "    34: \"Min TF title\",\n",
        "    35: \"Min TF URL\",\n",
        "    36: \"Max TF body\",\n",
        "    37: \"Max TF anchor\",\n",
        "    38: \"Max TF title\",\n",
        "    39: \"Max TF URL\",\n",
        "    40: \"Avg TF body\",\n",
        "    41: \"Avg TF anchor\",\n",
        "    42: \"Avg TF title\",\n",
        "    43: \"Avg TF URL\",\n",
        "    44: \"Body length\",\n",
        "    45: \"Anchor length\",\n",
        "    46: \"Title length\"\n",
        "}\n",
        "\n",
        "query_ids = df.columns.tolist()\n",
        "query_ids.remove(\"feature_number\")\n",
        "\n",
        "df[\"feature_name\"] = df[\"feature_number\"].map(feature_map)\n",
        "\n",
        "for qid in query_ids:\n",
        "    df_sorted = df.sort_values(by=qid, ascending=True)  # ascending=True p\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    plt.barh(df_sorted[\"feature_name\"], df_sorted[qid])\n",
        "    plt.title(f\"RankingSHAP: Feature Attribution for query {qid}\")\n",
        "    plt.xlabel(\"Attribution value\")\n",
        "    plt.ylabel(\"Feature name\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "AfMXYo_gp8DZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Mapping of features\n",
        "feature_map = {\n",
        "    1: \"TF-IDF body\",\n",
        "    2: \"TF-IDF anchor\",\n",
        "    3: \"TF-IDF title\",\n",
        "    4: \"TF-IDF URL\",\n",
        "    5: \"BM25 body\",\n",
        "    6: \"BM25 anchor\",\n",
        "    7: \"BM25 title\",\n",
        "    8: \"BM25 URL\",\n",
        "    9: \"LMIR.ABS body\",\n",
        "    10: \"LMIR.ABS anchor\",\n",
        "    11: \"LMIR.ABS title\",\n",
        "    12: \"LMIR.ABS URL\",\n",
        "    13: \"LMIR.DIR body\",\n",
        "    14: \"LMIR.DIR anchor\",\n",
        "    15: \"LMIR.DIR title\",\n",
        "    16: \"LMIR.DIR URL\",\n",
        "    17: \"LMIR.JM body\",\n",
        "    18: \"LMIR.JM anchor\",\n",
        "    19: \"LMIR.JM title\",\n",
        "    20: \"LMIR.JM URL\",\n",
        "    21: \"PageRank\",\n",
        "    22: \"Inlink number\",\n",
        "    23: \"Outlink number\",\n",
        "    24: \"URL slashes count\",\n",
        "    25: \"URL length\",\n",
        "    26: \"Child pages count\",\n",
        "    27: \"Query IDF\",\n",
        "    28: \"Sum TF body\",\n",
        "    29: \"Sum TF anchor\",\n",
        "    30: \"Sum TF title\",\n",
        "    31: \"Sum TF URL\",\n",
        "    32: \"Min TF body\",\n",
        "    33: \"Min TF anchor\",\n",
        "    34: \"Min TF title\",\n",
        "    35: \"Min TF URL\",\n",
        "    36: \"Max TF body\",\n",
        "    37: \"Max TF anchor\",\n",
        "    38: \"Max TF title\",\n",
        "    39: \"Max TF URL\",\n",
        "    40: \"Avg TF body\",\n",
        "    41: \"Avg TF anchor\",\n",
        "    42: \"Avg TF title\",\n",
        "    43: \"Avg TF URL\",\n",
        "    44: \"Body length\",\n",
        "    45: \"Anchor length\",\n",
        "    46: \"Title length\"\n",
        "}\n",
        "\n",
        "df = pd.read_csv(\"results/results_MQ2008/feature_attributes/rankingshap.csv\")\n",
        "\n",
        "query_cols = [col for col in df.columns if col != \"feature_number\"]\n",
        "\n",
        "df['mean_attribution'] = df[query_cols].mean(axis=1)\n",
        "\n",
        "df['feature_name'] = df['feature_number'].map(feature_map)\n",
        "\n",
        "df_sorted = df.sort_values(by='mean_attribution', ascending=True)  # ascending=True\n",
        "\n",
        "plt.figure(figsize=(12,10))\n",
        "plt.barh(df_sorted['feature_name'], df_sorted['mean_attribution'])\n",
        "plt.title(\"RankingSHAP: Global Feature Attribution (mean over 50 queries)\")\n",
        "plt.xlabel(\"Mean attribution value\")\n",
        "plt.ylabel(\"Feature name\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "clRmJOTCp_mk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Validation of top-k features through retraining and comparison\n",
        "\n",
        "We validate whether the top-k features identified using the global RankingSHAP aggregation\n",
        "are sufficient to maintain the modelâ€™s performance when used alone.  \n",
        "\n",
        "The steps are as follows:\n",
        "1ï¸ Retrain the ranking model using only the top-k features selected based on their global importance.  \n",
        "2ï¸ Evaluate the performance of this model and compare it to the baseline.  \n",
        "3ï¸ Retrain another model using all features (full model).  \n",
        "4ï¸ Compare the performance of this full model to that of the top-k feature model\n",
        "to demonstrate that the global feature ranking captures genuinely useful information for the model.\n"
      ],
      "metadata": {
        "id": "ifQw8O3IawX3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This cell computes the global mean attribution value for each feature\n",
        "using RankingSHAP results. It generates:\n",
        "- The list of top-k most important features based on mean attribution.\n",
        "- Several sets of k randomly selected features for comparison.\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Load the RankingSHAP attribution file\n",
        "df = pd.read_csv(\"results/results_MQ2008/feature_attributes/rankingshap.csv\")\n",
        "\n",
        "# Get the columns corresponding to query IDs\n",
        "query_cols = [col for col in df.columns if col != \"feature_number\"]\n",
        "\n",
        "# Compute the mean attribution value for each feature\n",
        "df['mean_attribution'] = df[query_cols].mean(axis=1)\n",
        "\n",
        "# Sort features by mean attribution in descending order\n",
        "df_sorted = df.sort_values(by='mean_attribution', ascending=False).reset_index(drop=True)\n",
        "\n",
        "# Generate list of sorted feature names (e.g., 'feature_39', 'feature_30', ...)\n",
        "sorted_feature_names = [f\"feature_{int(f)}\" for f in df_sorted['feature_number']]\n",
        "\n",
        "# Parameters\n",
        "k = 10  # you can change to k=20 if desired\n",
        "n_random_repeats = 5\n",
        "\n",
        "# Top-k features\n",
        "top_k_features = sorted_feature_names[:k]\n",
        "\n",
        "# Generate random-k feature sets\n",
        "random_k_features_list = [\n",
        "    random.sample(sorted_feature_names, k) for _ in range(n_random_repeats)\n",
        "]\n",
        "\n",
        "# Display results\n",
        "print(f\"\\n=== Top-{k} Features ===\")\n",
        "print(top_k_features)\n",
        "\n",
        "print(f\"\\n=== Random-{k} Features (repeats={n_random_repeats}) ===\")\n",
        "for i, feats in enumerate(random_k_features_list):\n",
        "    print(f\"Random set {i+1}: {feats}\")\n"
      ],
      "metadata": {
        "id": "SebLipFWauUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This cell:\n",
        "- Loads the training, validation, and test datasets in LETOR format.\n",
        "- Loads the RankingSHAP global feature ranking.\n",
        "- Selects the top-k features based on mean attribution.\n",
        "- Generates several sets of k random features for comparison.\n",
        "- Filters the datasets to keep only the selected features (top-k and random-k).\n",
        "- Saves these filtered datasets as CSV files for future experiments.\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "random.seed(42)\n",
        "\n",
        "# Function to read a LETOR-style file\n",
        "def read_letor_file(filepath):\n",
        "    X, y, qids = [], [], []\n",
        "    with open(filepath, \"r\") as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split()\n",
        "            if len(parts) < 2:\n",
        "                continue  # Skip invalid lines\n",
        "            y_val = int(parts[0])\n",
        "            qid = int(parts[1].split(\":\")[1])\n",
        "            features = []\n",
        "            for item in parts[2:]:\n",
        "                if item.startswith(\"#\"):\n",
        "                    break  # Ignore comments\n",
        "                _, val = item.split(\":\")\n",
        "                features.append(float(val))\n",
        "            X.append(features)\n",
        "            y.append(y_val)\n",
        "            qids.append(qid)\n",
        "    return np.array(X), np.array(y), np.array(qids)\n",
        "\n",
        "# Load train+valid and test sets\n",
        "X_train, y_train, qids_train = read_letor_file(\"data/MQ2008/Fold1/train.txt\")\n",
        "X_test, y_test, qids_test = read_letor_file(\"data/MQ2008/Fold1/test_1.txt\")\n",
        "\n",
        "# Create DataFrames\n",
        "feature_names = [f\"feature_{i+1}\" for i in range(X_train.shape[1])]\n",
        "train_df = pd.DataFrame(X_train, columns=feature_names)\n",
        "train_df['label'] = y_train\n",
        "train_df['qid'] = qids_train\n",
        "\n",
        "test_df = pd.DataFrame(X_test, columns=feature_names)\n",
        "test_df['label'] = y_test\n",
        "test_df['qid'] = qids_test\n",
        "\n",
        "# Load feature ranking from RankingSHAP\n",
        "df_rank = pd.read_csv(\"results/results_MQ2008/feature_attributes/rankingshap.csv\")\n",
        "query_cols = [col for col in df_rank.columns if col != \"feature_number\"]\n",
        "df_rank['mean_attribution'] = df_rank[query_cols].mean(axis=1)\n",
        "df_rank_sorted = df_rank.sort_values(by='mean_attribution', ascending=False).reset_index(drop=True)\n",
        "sorted_feature_names = [f\"feature_{int(f)}\" for f in df_rank_sorted['feature_number']]\n",
        "\n",
        "# Parameters\n",
        "k = 10\n",
        "n_random_repeats = 5\n",
        "\n",
        "# Select top-k features\n",
        "top_k_features = sorted_feature_names[:k]\n",
        "\n",
        "# Generate random-k feature sets\n",
        "random_k_features_list = [\n",
        "    random.sample(feature_names, k) for _ in range(n_random_repeats)\n",
        "]\n",
        "\n",
        "# Function to filter dataset columns\n",
        "def filter_dataset(df, feature_list):\n",
        "    cols_to_keep = ['label', 'qid'] + feature_list\n",
        "    return df[cols_to_keep].copy()\n",
        "\n",
        "# Create filtered datasets\n",
        "train_top_k = filter_dataset(train_df, top_k_features)\n",
        "test_top_k = filter_dataset(test_df, top_k_features)\n",
        "\n",
        "train_random_k_list = [filter_dataset(train_df, feats) for feats in random_k_features_list]\n",
        "test_random_k_list = [filter_dataset(test_df, feats) for feats in random_k_features_list]\n",
        "\n",
        "# Display selected features\n",
        "print(f\"âœ… Top-{k} features selected:\")\n",
        "print(top_k_features)\n",
        "\n",
        "for i, feats in enumerate(random_k_features_list):\n",
        "    print(f\"Random-{k} set {i+1}: {feats}\")\n",
        "\n",
        "# Save datasets to CSV files\n",
        "train_top_k.to_csv(\"train_top_k.csv\", index=False)\n",
        "test_top_k.to_csv(\"test_top_k.csv\", index=False)\n",
        "for i, (train_r, test_r) in enumerate(zip(train_random_k_list, test_random_k_list)):\n",
        "    train_r.to_csv(f\"train_random_k_{i+1}.csv\", index=False)\n",
        "    test_r.to_csv(f\"test_random_k_{i+1}.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "Ini_iawvcotA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "gKoNEJ0lb2_O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This cell:\n",
        "- Defines parameters for fair comparison of models.\n",
        "- Implements a function to train a LambdaMART LightGBM model.\n",
        "- Trains a model using the top-k selected features.\n",
        "- Trains multiple models using random-k selected features.\n",
        "\"\"\"\n",
        "\n",
        "import lightgbm as lgb\n",
        "import numpy as np\n",
        "\n",
        "# General parameters (fixed for fair comparison)\n",
        "num_leaves = 31\n",
        "n_estimators = 100\n",
        "learning_rate = 0.1\n",
        "random_seed = 42\n",
        "\n",
        "# Function to train a LambdaMART LightGBM model\n",
        "def train_lgb_ranker(train_df):\n",
        "    # Separate features, labels, and query groups\n",
        "    X_train = train_df.drop(columns=['label', 'qid'])\n",
        "    y_train = train_df['label']\n",
        "    group_train = train_df.groupby('qid').size().to_numpy()\n",
        "\n",
        "    # Create LightGBM dataset\n",
        "    lgb_train = lgb.Dataset(X_train, label=y_train, group=group_train)\n",
        "\n",
        "    # Train the model\n",
        "    model = lgb.train(\n",
        "        {\n",
        "            'objective': 'lambdarank',\n",
        "            'metric': 'ndcg',\n",
        "            'learning_rate': learning_rate,\n",
        "            'num_leaves': num_leaves,\n",
        "            'verbose': -1,\n",
        "            'seed': random_seed\n",
        "        },\n",
        "        lgb_train,\n",
        "        num_boost_round=n_estimators\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# Train the Top-k model\n",
        "print(\"ðŸš€ Training Top-k model...\")\n",
        "model_top_k = train_lgb_ranker(train_top_k)\n",
        "\n",
        "# Train the Random-k models\n",
        "model_random_k_list = []\n",
        "for i, train_r in enumerate(train_random_k_list):\n",
        "    print(f\"ðŸš€ Training Random-k model {i+1}...\")\n",
        "    model_r = train_lgb_ranker(train_r)\n",
        "    model_random_k_list.append(model_r)\n"
      ],
      "metadata": {
        "id": "-UJ3z3n3fhzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This cell:\n",
        "- Defines a function to generate predictions grouped by query ID (qid).\n",
        "- Defines a function to compute ranking metrics: NDCG@10, MAP, and Precision@10.\n",
        "- Evaluates these metrics on the model trained with top-k features.\n",
        "- Evaluates these metrics on models trained with random-k features.\n",
        "\"\"\"\n",
        "\n",
        "from sklearn.metrics import average_precision_score\n",
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "# Function to generate predictions grouped by qid\n",
        "def predict_and_group(model, test_df):\n",
        "    X_test = test_df.drop(columns=['label', 'qid'])\n",
        "    y_test = test_df['label']\n",
        "    qid_test = test_df['qid']\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    grouped = []\n",
        "    for qid in np.unique(qid_test):\n",
        "        mask = qid_test == qid\n",
        "        grouped.append({\n",
        "            'qid': qid,\n",
        "            'y_true': y_test[mask].to_numpy(),\n",
        "            'y_pred': y_pred[mask]\n",
        "        })\n",
        "    return grouped\n",
        "\n",
        "# Function to compute ranking metrics\n",
        "def eval_ranking(grouped_data, k=10):\n",
        "    ndcg_list = []\n",
        "    map_list = []\n",
        "    prec_list = []\n",
        "\n",
        "    for group in grouped_data:\n",
        "        y_true = np.ravel(group['y_true'])\n",
        "        y_pred = np.ravel(group['y_pred'])\n",
        "\n",
        "        if len(y_true) < 2:\n",
        "            continue  # Skip groups with insufficient documents\n",
        "\n",
        "        order = np.argsort(-y_pred)\n",
        "        y_true_sorted = y_true[order]\n",
        "\n",
        "        # NDCG@k\n",
        "        dcg = np.sum((2 ** y_true_sorted[:k] - 1) / np.log2(np.arange(2, 2 + len(y_true_sorted[:k]))))\n",
        "        ideal_order = np.sort(y_true)[::-1]\n",
        "        ideal_dcg = np.sum((2 ** ideal_order[:k] - 1) / np.log2(np.arange(2, 2 + len(ideal_order[:k]))))\n",
        "        ndcg = dcg / ideal_dcg if ideal_dcg > 0 else 0.0\n",
        "        ndcg_list.append(ndcg)\n",
        "\n",
        "        # MAP\n",
        "        try:\n",
        "            if np.sum(y_true) > 0:\n",
        "                ap = average_precision_score(y_true, y_pred)\n",
        "            else:\n",
        "                ap = 0.0\n",
        "        except ValueError:\n",
        "            ap = 0.0\n",
        "        map_list.append(ap)\n",
        "\n",
        "        # Precision@k\n",
        "        prec = np.sum(y_true_sorted[:k]) / k\n",
        "        prec_list.append(prec)\n",
        "\n",
        "    return {\n",
        "        'NDCG@10': np.mean(ndcg_list) if ndcg_list else 0.0,\n",
        "        'MAP': np.mean(map_list) if map_list else 0.0,\n",
        "        'Precision@10': np.mean(prec_list) if prec_list else 0.0\n",
        "    }\n",
        "\n",
        "# Evaluate Top-k model\n",
        "print(\"âœ… Evaluating Top-k model...\")\n",
        "grouped_top = predict_and_group(model_top_k, test_top_k)\n",
        "results_top = eval_ranking(grouped_top)\n",
        "print(\"Top-k results:\", results_top)\n",
        "\n",
        "# Evaluate Random-k models\n",
        "results_random_list = []\n",
        "for i, (model_r, test_r) in enumerate(zip(model_random_k_list, test_random_k_list)):\n",
        "    print(f\"âœ… Evaluating Random-k model {i+1}...\")\n",
        "    grouped_r = predict_and_group(model_r, test_r)\n",
        "    with warnings.catch_warnings():\n",
        "        warnings.simplefilter(\"ignore\", category=UserWarning)\n",
        "        results_r = eval_ranking(grouped_r)\n",
        "    results_random_list.append(results_r)\n",
        "    print(f\"Random-k {i+1} results:\", results_r)\n"
      ],
      "metadata": {
        "id": "NvIdfqtve80j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This cell:\n",
        "- Defines a function to plot a bar chart comparing NDCG@10 for the top-k model and the mean of random-k models, including error bars for the random-k standard deviation.\n",
        "- Extracts NDCG@10, MAP, and Precision@10 values from the random-k evaluation results.\n",
        "- Computes summary statistics (mean and standard deviation) for these metrics.\n",
        "- Visualizes the NDCG@10 comparison in a bar chart.\n",
        "\"\"\"\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_bar_ndcg(top_val, random_mean, random_std):\n",
        "    labels = ['Top-k', 'Random-k mean']\n",
        "    ndcg_values = [top_val, random_mean]\n",
        "    errors = [0, random_std]  # No error bar for Top-k\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(6, 4))\n",
        "    ax.bar(labels, ndcg_values, yerr=errors, capsize=5, color=['tab:blue', 'tab:orange'])\n",
        "    ax.set_ylabel('NDCG@10')\n",
        "    ax.set_title('NDCG@10: Top-k vs Random-k')\n",
        "    ax.set_ylim(0, 1.0)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Extract NDCG@10, MAP, and Precision@10 values from random-k results\n",
        "ndcg_vals = [r['NDCG@10'] for r in results_random_list]\n",
        "map_vals = [r['MAP'] for r in results_random_list]\n",
        "prec_vals = [r['Precision@10'] for r in results_random_list]\n",
        "\n",
        "# Compute summary statistics\n",
        "summary_random = {\n",
        "    'NDCG@10 mean': np.mean(ndcg_vals),\n",
        "    'NDCG@10 std': np.std(ndcg_vals),\n",
        "    'MAP mean': np.mean(map_vals),\n",
        "    'MAP std': np.std(map_vals),\n",
        "    'Precision@10 mean': np.mean(prec_vals),\n",
        "    'Precision@10 std': np.std(prec_vals)\n",
        "}\n",
        "\n",
        "# Visualize NDCG@10 comparison\n",
        "plot_bar_ndcg(results_top['NDCG@10'], summary_random['NDCG@10 mean'], summary_random['NDCG@10 std'])\n"
      ],
      "metadata": {
        "id": "ewEE8lP6gDmw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This cell:\n",
        "- Defines a generic plotting function to compare Top-k and Random-k models for any ranking metric.\n",
        "- Generates bar charts for MAP and Precision@10, showing the Top-k value and the mean Â± standard deviation of Random-k.\n",
        "\"\"\"\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_bar_metric(metric_name, top_val, random_mean, random_std):\n",
        "    labels = ['Top-k', 'Random-k mean']\n",
        "    values = [top_val, random_mean]\n",
        "    errors = [0, random_std]\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(6, 4))\n",
        "    ax.bar(labels, values, yerr=errors, capsize=5, color=['tab:green', 'tab:purple'])\n",
        "    ax.set_ylabel(metric_name)\n",
        "    ax.set_title(f'{metric_name}: Top-k vs Random-k')\n",
        "    ax.set_ylim(0, 1.0)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Generate plots for MAP and Precision@10\n",
        "plot_bar_metric('MAP', results_top['MAP'], summary_random['MAP mean'], summary_random['MAP std'])\n",
        "plot_bar_metric('Precision@10', results_top['Precision@10'], summary_random['Precision@10 mean'], summary_random['Precision@10 std'])\n"
      ],
      "metadata": {
        "id": "Re21Vg3IgPeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This cell:\n",
        "- Defines a function to compare the performance of the Top-k model to the full-feature model for a given metric.\n",
        "- Trains a model using all features (full-feature model).\n",
        "- Evaluates the full-feature model on the full test set.\n",
        "- Computes and prints the percentage of full performance retained by the Top-k model for NDCG@10, MAP, and Precision@10.\n",
        "\"\"\"\n",
        "\n",
        "def compare_to_full(top_result, full_result, metric='NDCG@10'):\n",
        "    if full_result > 0:\n",
        "        perc = 100 * (top_result / full_result)\n",
        "    else:\n",
        "        perc = 0.0\n",
        "    print(f\"âœ… Top-k retains {perc:.2f}% of full {metric} performance (Top-k: {top_result:.4f}, Full: {full_result:.4f})\")\n",
        "    return perc\n",
        "\n",
        "# Train the full-feature model\n",
        "model_full = train_lgb_ranker(train_df)  # train_df: full training dataset with all features\n",
        "\n",
        "# Evaluate the full-feature model on the full test set\n",
        "grouped_full = predict_and_group(model_full, test_df)  # test_df: full test dataset with all features\n",
        "results_full = eval_ranking(grouped_full)\n",
        "\n",
        "# Compare Top-k performance to full-feature model\n",
        "perc_ndcg = compare_to_full(results_top['NDCG@10'], results_full['NDCG@10'], 'NDCG@10')\n",
        "perc_map = compare_to_full(results_top['MAP'], results_full['MAP'], 'MAP')\n",
        "perc_prec = compare_to_full(results_top['Precision@10'], results_full['Precision@10'], 'Precision@10')\n"
      ],
      "metadata": {
        "id": "z15AkcukgqE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_top_random(top_val, random_mean, random_std, metric):\n",
        "    diff = top_val - random_mean\n",
        "    print(f\"âœ… Top-k {metric}: {top_val:.4f} | Random-k mean: {random_mean:.4f} Â± {random_std:.4f} | Diff: {diff:.4f}\")\n",
        "    return diff\n",
        "\n",
        "# Compare NDCG@10\n",
        "diff_ndcg = compare_top_random(\n",
        "    results_top['NDCG@10'],\n",
        "    summary_random['NDCG@10 mean'],\n",
        "    summary_random['NDCG@10 std'],\n",
        "    'NDCG@10'\n",
        ")\n",
        "\n",
        "# Compare MAP\n",
        "diff_map = compare_top_random(\n",
        "    results_top['MAP'],\n",
        "    summary_random['MAP mean'],\n",
        "    summary_random['MAP std'],\n",
        "    'MAP'\n",
        ")\n",
        "\n",
        "# Compare Precision@10\n",
        "diff_prec = compare_top_random(\n",
        "    results_top['Precision@10'],\n",
        "    summary_random['Precision@10 mean'],\n",
        "    summary_random['Precision@10 std'],\n",
        "    'Precision@10'\n",
        ")\n"
      ],
      "metadata": {
        "id": "aPvaYl7FguUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Validation of top-k features with varying k\n",
        "\n",
        "We now repeat the same experiments while varying the value of k.  \n",
        "This allows us to assess how the number of top features impacts model performance,  \n",
        "and whether a larger or smaller subset of features provides a better balance between simplicity and effectiveness.\n",
        "\n",
        "The process includes:\n",
        "- Retraining models using top-k features for different values of k.\n",
        "- Comparing their performance to models using k randomly selected features.\n",
        "- Evaluating how much of the full modelâ€™s performance is retained as k changes.\n"
      ],
      "metadata": {
        "id": "IcjdNEtxkaIa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This cell:\n",
        "- Loads training and test data in LETOR format.\n",
        "- Loads the global RankingSHAP feature ranking.\n",
        "- Iterates over different values of k (from 5 to 20).\n",
        "- For each k:\n",
        "    - Selects the top-k features based on global RankingSHAP attribution.\n",
        "    - Generates multiple random-k feature sets for comparison.\n",
        "    - Trains a LightGBM LambdaMART model on top-k features.\n",
        "    - Trains LightGBM models on each random-k feature set.\n",
        "    - Saves the corresponding datasets for future analysis.\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import lightgbm as lgb\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "# Function to read a LETOR-style file\n",
        "def read_letor_file(filepath):\n",
        "    X, y, qids = [], [], []\n",
        "    with open(filepath, \"r\") as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split()\n",
        "            if len(parts) < 2:\n",
        "                continue  # Skip invalid lines\n",
        "            y_val = int(parts[0])\n",
        "            qid = int(parts[1].split(\":\")[1])\n",
        "            features = []\n",
        "            for item in parts[2:]:\n",
        "                if item.startswith(\"#\"):\n",
        "                    break  # Ignore comments\n",
        "                _, val = item.split(\":\")\n",
        "                features.append(float(val))\n",
        "            X.append(features)\n",
        "            y.append(y_val)\n",
        "            qids.append(qid)\n",
        "    return np.array(X), np.array(y), np.array(qids)\n",
        "\n",
        "# Load train and test sets\n",
        "X_train, y_train, qids_train = read_letor_file(\"data/MQ2008/Fold1/train.txt\")\n",
        "X_test, y_test, qids_test = read_letor_file(\"data/MQ2008/Fold1/test_1.txt\")\n",
        "\n",
        "feature_names = [f\"feature_{i+1}\" for i in range(X_train.shape[1])]\n",
        "\n",
        "train_df = pd.DataFrame(X_train, columns=feature_names)\n",
        "train_df['label'] = y_train\n",
        "train_df['qid'] = qids_train\n",
        "\n",
        "test_df = pd.DataFrame(X_test, columns=feature_names)\n",
        "test_df['label'] = y_test\n",
        "test_df['qid'] = qids_test\n",
        "\n",
        "# Load feature ranking\n",
        "df_rank = pd.read_csv(\"results/results_MQ2008/feature_attributes/rankingshap.csv\")\n",
        "query_cols = [col for col in df_rank.columns if col != \"feature_number\"]\n",
        "df_rank['mean_attribution'] = df_rank[query_cols].mean(axis=1)\n",
        "df_rank_sorted = df_rank.sort_values(by='mean_attribution', ascending=False).reset_index(drop=True)\n",
        "sorted_feature_names = [f\"feature_{int(f)}\" for f in df_rank_sorted['feature_number']]\n",
        "\n",
        "# Function to filter dataset columns\n",
        "def filter_dataset(df, feature_list):\n",
        "    cols_to_keep = ['label', 'qid'] + feature_list\n",
        "    return df[cols_to_keep].copy()\n",
        "\n",
        "# Function to train a LightGBM LambdaMART model\n",
        "def train_lgb_ranker(train_df):\n",
        "    X_train = train_df.drop(columns=['label', 'qid'])\n",
        "    y_train = train_df['label']\n",
        "    group_train = train_df.groupby('qid').size().to_numpy()\n",
        "    lgb_train = lgb.Dataset(X_train, label=y_train, group=group_train)\n",
        "    model = lgb.train(\n",
        "        {\n",
        "            'objective': 'lambdarank',\n",
        "            'metric': 'ndcg',\n",
        "            'learning_rate': 0.1,\n",
        "            'num_leaves': 31,\n",
        "            'verbose': -1,\n",
        "            'seed': 42\n",
        "        },\n",
        "        lgb_train,\n",
        "        num_boost_round=100\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# Iterate over k values\n",
        "for k in range(5, 21):\n",
        "    print(f\"\\nðŸ”¹ðŸ”¹ Processing k={k} ðŸ”¹ðŸ”¹\")\n",
        "\n",
        "    top_k_features = sorted_feature_names[:k]\n",
        "    random_k_features_list = [random.sample(feature_names, k) for _ in range(5)]\n",
        "\n",
        "    # Top-k\n",
        "    train_top_k = filter_dataset(train_df, top_k_features)\n",
        "    test_top_k = filter_dataset(test_df, top_k_features)\n",
        "    print(f\"âœ… Top-{k} features: {top_k_features}\")\n",
        "    model_top_k = train_lgb_ranker(train_top_k)\n",
        "\n",
        "    # Save datasets if needed\n",
        "    train_top_k.to_csv(f\"train_top_k_{k}.csv\", index=False)\n",
        "    test_top_k.to_csv(f\"test_top_k_{k}.csv\", index=False)\n",
        "\n",
        "    # Random-k\n",
        "    model_random_k_list = []\n",
        "    for i, feats in enumerate(random_k_features_list):\n",
        "        print(f\"   ðŸš€ Training Random-k {i+1}: {feats}\")\n",
        "        train_r = filter_dataset(train_df, feats)\n",
        "        test_r = filter_dataset(test_df, feats)\n",
        "        model_r = train_lgb_ranker(train_r)\n",
        "        model_random_k_list.append(model_r)\n",
        "\n",
        "        # Save datasets if needed\n",
        "        train_r.to_csv(f\"train_random_k{k}_{i+1}.csv\", index=False)\n",
        "        test_r.to_csv(f\"test_random_k{k}_{i+1}.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "ydE3jeXhjLl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This cell:\n",
        "- Trains LightGBM LambdaMART models for different values of k (from 5 to 20) using both top-k and random-k feature sets.\n",
        "- Evaluates each model on the corresponding test set.\n",
        "- Computes NDCG@10, MAP, and Precision@10 for both top-k and random-k models.\n",
        "- Aggregates and summarizes the results across all k values.\n",
        "- Saves the results as a CSV file for further analysis.\n",
        "\"\"\"\n",
        "\n",
        "from sklearn.metrics import average_precision_score\n",
        "import numpy as np\n",
        "import warnings\n",
        "import lightgbm as lgb\n",
        "\n",
        "# Dictionaries to store models and test sets\n",
        "models_top_k = {}\n",
        "tests_top_k = {}\n",
        "models_random_k = {}\n",
        "tests_random_k = {}\n",
        "\n",
        "for k in range(5, 21):\n",
        "    print(f\"\\nâš¡ Preparing k={k}\")\n",
        "\n",
        "    # Top-k features\n",
        "    top_k_features = sorted_feature_names[:k]\n",
        "    train_top = filter_dataset(train_df, top_k_features)\n",
        "    test_top = filter_dataset(test_df, top_k_features)\n",
        "\n",
        "    model_top = train_lgb_ranker(train_top)\n",
        "\n",
        "    models_top_k[k] = model_top\n",
        "    tests_top_k[k] = test_top\n",
        "\n",
        "    # Random-k features\n",
        "    random_models = []\n",
        "    random_tests = []\n",
        "    random_k_features_list = [random.sample(feature_names, k) for _ in range(5)]\n",
        "\n",
        "    for feats in random_k_features_list:\n",
        "        train_rand = filter_dataset(train_df, feats)\n",
        "        test_rand = filter_dataset(test_df, feats)\n",
        "\n",
        "        model_rand = train_lgb_ranker(train_rand)\n",
        "\n",
        "        random_models.append(model_rand)\n",
        "        random_tests.append(test_rand)\n",
        "\n",
        "    models_random_k[k] = random_models\n",
        "    tests_random_k[k] = random_tests\n",
        "\n",
        "# Functions for prediction and evaluation\n",
        "def predict_and_group(model, test_df):\n",
        "    X_test = test_df.drop(columns=['label', 'qid'])\n",
        "    y_test = test_df['label']\n",
        "    qid_test = test_df['qid']\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    grouped = []\n",
        "    for qid in np.unique(qid_test):\n",
        "        mask = qid_test == qid\n",
        "        grouped.append({\n",
        "            'qid': qid,\n",
        "            'y_true': y_test[mask].to_numpy(),\n",
        "            'y_pred': y_pred[mask]\n",
        "        })\n",
        "    return grouped\n",
        "\n",
        "def eval_ranking(grouped_data, k=10):\n",
        "    ndcg_list = []\n",
        "    map_list = []\n",
        "    prec_list = []\n",
        "\n",
        "    for group in grouped_data:\n",
        "        y_true = np.ravel(group['y_true'])\n",
        "        y_pred = np.ravel(group['y_pred'])\n",
        "\n",
        "        if len(y_true) < 2:\n",
        "            continue\n",
        "\n",
        "        order = np.argsort(-y_pred)\n",
        "        y_true_sorted = y_true[order]\n",
        "\n",
        "        # NDCG@k\n",
        "        dcg = np.sum((2 ** y_true_sorted[:k] - 1) / np.log2(np.arange(2, 2 + len(y_true_sorted[:k]))))\n",
        "        ideal_order = np.sort(y_true)[::-1]\n",
        "        ideal_dcg = np.sum((2 ** ideal_order[:k] - 1) / np.log2(np.arange(2, 2 + len(ideal_order[:k]))))\n",
        "        ndcg = dcg / ideal_dcg if ideal_dcg > 0 else 0.0\n",
        "        ndcg_list.append(ndcg)\n",
        "\n",
        "        # MAP\n",
        "        try:\n",
        "            if np.sum(y_true) > 0:\n",
        "                ap = average_precision_score(y_true, y_pred)\n",
        "            else:\n",
        "                ap = 0.0\n",
        "        except ValueError:\n",
        "            ap = 0.0\n",
        "        map_list.append(ap)\n",
        "\n",
        "        # Precision@k\n",
        "        prec = np.sum(y_true_sorted[:k]) / k\n",
        "        prec_list.append(prec)\n",
        "\n",
        "    return {\n",
        "        'NDCG@10': np.mean(ndcg_list) if ndcg_list else 0.0,\n",
        "        'MAP': np.mean(map_list) if map_list else 0.0,\n",
        "        'Precision@10': np.mean(prec_list) if prec_list else 0.0\n",
        "    }\n",
        "\n",
        "# Evaluate and collect results\n",
        "all_results = []\n",
        "\n",
        "for k in range(5, 21):\n",
        "    print(f\"\\nðŸ“Š Evaluating k={k} Top-k model...\")\n",
        "    grouped_top = predict_and_group(models_top_k[k], tests_top_k[k])\n",
        "    results_top = eval_ranking(grouped_top)\n",
        "    print(f\"Top-{k} results: {results_top}\")\n",
        "\n",
        "    k_result = {\n",
        "        'k': k,\n",
        "        'Top NDCG@10': results_top['NDCG@10'],\n",
        "        'Top MAP': results_top['MAP'],\n",
        "        'Top Precision@10': results_top['Precision@10']\n",
        "    }\n",
        "\n",
        "    ndcg_random = []\n",
        "    map_random = []\n",
        "    prec_random = []\n",
        "\n",
        "    for i, (model_r, test_r) in enumerate(zip(models_random_k[k], tests_random_k[k])):\n",
        "        print(f\"   âœ… Evaluating Random-k {i+1} model...\")\n",
        "        grouped_r = predict_and_group(model_r, test_r)\n",
        "        with warnings.catch_warnings():\n",
        "            warnings.simplefilter(\"ignore\", category=UserWarning)\n",
        "            results_r = eval_ranking(grouped_r)\n",
        "        print(f\"   Random-{k} {i+1} results: {results_r}\")\n",
        "\n",
        "        ndcg_random.append(results_r['NDCG@10'])\n",
        "        map_random.append(results_r['MAP'])\n",
        "        prec_random.append(results_r['Precision@10'])\n",
        "\n",
        "    k_result.update({\n",
        "        'Random NDCG@10 mean': np.mean(ndcg_random),\n",
        "        'Random NDCG@10 std': np.std(ndcg_random),\n",
        "        'Random MAP mean': np.mean(map_random),\n",
        "        'Random MAP std': np.std(map_random),\n",
        "        'Random Precision@10 mean': np.mean(prec_random),\n",
        "        'Random Precision@10 std': np.std(prec_random)\n",
        "    })\n",
        "\n",
        "    all_results.append(k_result)\n",
        "\n",
        "# Summarize and save results\n",
        "df_results = pd.DataFrame(all_results)\n",
        "print(\"\\nðŸ“ˆ Overall results:\")\n",
        "print(df_results)\n",
        "\n",
        "df_results.to_csv(\"ranking_summary_all_k.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "iEKJJcw7jrUC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This cell:\n",
        "- Trains a full-feature LightGBM LambdaMART model once.\n",
        "- Evaluates the full-feature model on the full test set.\n",
        "- Defines a function to compare Top-k model performance to the full-feature model for each metric.\n",
        "- Updates the results to include the percentage of full model performance retained by each Top-k model.\n",
        "- Saves the final summary as a CSV file.\n",
        "\"\"\"\n",
        "\n",
        "# Train the full-feature model once\n",
        "print(\"ðŸš€ Training full feature model...\")\n",
        "model_full = train_lgb_ranker(train_df)\n",
        "\n",
        "# Evaluate on the full test set\n",
        "grouped_full = predict_and_group(model_full, test_df)\n",
        "results_full = eval_ranking(grouped_full)\n",
        "\n",
        "print(f\"âœ… Full model results: {results_full}\")\n",
        "\n",
        "# Function to compare Top-k results to full-feature model\n",
        "def compare_to_full(top_result, full_result, metric='NDCG@10'):\n",
        "    if full_result > 0:\n",
        "        perc = 100 * (top_result / full_result)\n",
        "    else:\n",
        "        perc = 0.0\n",
        "    print(f\"âœ… Top-k retains {perc:.2f}% of full {metric} performance (Top-k: {top_result:.4f}, Full: {full_result:.4f})\")\n",
        "    return perc\n",
        "\n",
        "# Update results with comparisons to full model\n",
        "for res in all_results:\n",
        "    res['Top NDCG@10 % full'] = compare_to_full(res['Top NDCG@10'], results_full['NDCG@10'], 'NDCG@10')\n",
        "    res['Top MAP % full'] = compare_to_full(res['Top MAP'], results_full['MAP'], 'MAP')\n",
        "    res['Top Precision@10 % full'] = compare_to_full(res['Top Precision@10'], results_full['Precision@10'], 'Precision@10')\n",
        "\n",
        "# Convert to DataFrame for final summary\n",
        "df_results_full = pd.DataFrame(all_results)\n",
        "print(\"\\nðŸ“ˆ Overall results with full model comparison:\")\n",
        "print(df_results_full)\n",
        "\n",
        "# Save to CSV\n",
        "df_results_full.to_csv(\"ranking_summary_all_k_with_full.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "qcLTPW0Ql20V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This cell:\n",
        "- Defines a function to plot the performance of Top-k models as a percentage of the full model's performance.\n",
        "- Generates a line chart showing NDCG@10, MAP, and Precision@10 percentages across different values of k.\n",
        "\"\"\"\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_topk_vs_full(df_results):\n",
        "    plt.figure(figsize=(8, 5))\n",
        "\n",
        "    plt.plot(df_results['k'], df_results['Top NDCG@10 % full'], marker='o', label='NDCG@10 % full')\n",
        "    plt.plot(df_results['k'], df_results['Top MAP % full'], marker='s', label='MAP % full')\n",
        "    plt.plot(df_results['k'], df_results['Top Precision@10 % full'], marker='^', label='Precision@10 % full')\n",
        "\n",
        "    plt.axhline(y=100, color='gray', linestyle='--', linewidth=1, label='Full model = 100%')\n",
        "\n",
        "    plt.xlabel('Number of Top-k features')\n",
        "    plt.ylabel('Performance (% of full model)')\n",
        "    plt.title('Top-k performance vs full model')\n",
        "    plt.ylim(90, 105)  # adjust if needed\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Call the function with the final results DataFrame\n",
        "plot_topk_vs_full(df_results_full)\n"
      ],
      "metadata": {
        "id": "T74WMkTftsgC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This cell:\n",
        "- Defines a function to compare the performance of random-k models to the corresponding top-k model for each metric.\n",
        "- Updates the results with the percentage of top-k performance achieved by random-k models and the absolute difference.\n",
        "- Prints the detailed comparison.\n",
        "- Saves the final results including these comparisons as a CSV file.\n",
        "\"\"\"\n",
        "\n",
        "def compare_random_to_top(top_val, random_mean, random_std, metric, k):\n",
        "    if top_val > 0:\n",
        "        perc = 100 * (random_mean / top_val)\n",
        "    else:\n",
        "        perc = 0.0\n",
        "    diff = random_mean - top_val\n",
        "    print(f\"âœ… k={k} | Random-k {metric} mean: {random_mean:.4f} Â± {random_std:.4f} | Top-k: {top_val:.4f} | Diff: {diff:.4f} | Random-k = {perc:.2f}% of Top-k\")\n",
        "    return perc, diff\n",
        "\n",
        "# Update results with random-k vs top-k comparisons\n",
        "for res in all_results:\n",
        "    perc_ndcg, diff_ndcg = compare_random_to_top(\n",
        "        res['Top NDCG@10'],\n",
        "        res['Random NDCG@10 mean'],\n",
        "        res['Random NDCG@10 std'],\n",
        "        'NDCG@10',\n",
        "        res['k']\n",
        "    )\n",
        "    perc_map, diff_map = compare_random_to_top(\n",
        "        res['Top MAP'],\n",
        "        res['Random MAP mean'],\n",
        "        res['Random MAP std'],\n",
        "        'MAP',\n",
        "        res['k']\n",
        "    )\n",
        "    perc_prec, diff_prec = compare_random_to_top(\n",
        "        res['Top Precision@10'],\n",
        "        res['Random Precision@10 mean'],\n",
        "        res['Random Precision@10 std'],\n",
        "        'Precision@10',\n",
        "        res['k']\n",
        "    )\n",
        "\n",
        "    res['Random NDCG@10 vs Top %'] = perc_ndcg\n",
        "    res['Random NDCG@10 vs Top diff'] = diff_ndcg\n",
        "    res['Random MAP vs Top %'] = perc_map\n",
        "    res['Random MAP vs Top diff'] = diff_map\n",
        "    res['Random Precision@10 vs Top %'] = perc_prec\n",
        "    res['Random Precision@10 vs Top diff'] = diff_prec\n",
        "\n",
        "# Final summary as DataFrame\n",
        "df_results_final = pd.DataFrame(all_results)\n",
        "print(\"\\nðŸ“ˆ Overall results with Random-k vs Top-k comparison (% and diff):\")\n",
        "print(df_results_final)\n",
        "\n",
        "# Save final results\n",
        "df_results_final.to_csv(\"ranking_summary_all_k_random_vs_top_pct.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "ZtOjNxHRmVlD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This cell:\n",
        "- Defines a function to plot the performance of Top-k models as a percentage relative to the mean performance of Random-k models for each metric.\n",
        "- Generates a line chart showing NDCG@10, MAP, and Precision@10 Top-k vs Random-k percentages across different k values.\n",
        "\"\"\"\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_topk_vs_randomk(df_results):\n",
        "    plt.figure(figsize=(8,5))\n",
        "\n",
        "    plt.plot(df_results['k'], df_results['Top NDCG@10 vs Random %'], marker='o', label='NDCG@10 Top vs Random %')\n",
        "    plt.plot(df_results['k'], df_results['Top MAP vs Random %'], marker='s', label='MAP Top vs Random %')\n",
        "    plt.plot(df_results['k'], df_results['Top Precision@10 vs Random %'], marker='^', label='Precision@10 Top vs Random %')\n",
        "\n",
        "    plt.axhline(y=100, color='gray', linestyle='--', linewidth=1, label='Random mean = 100%')\n",
        "\n",
        "    plt.xlabel('Number of Top-k features')\n",
        "    plt.ylabel('Top-k performance (% of random-k mean)')\n",
        "    plt.title('Top-k performance vs Random-k mean')\n",
        "    plt.ylim(min(df_results[['Top NDCG@10 vs Random %', 'Top MAP vs Random %', 'Top Precision@10 vs Random %']].min()) - 2,\n",
        "             max(df_results[['Top NDCG@10 vs Random %', 'Top MAP vs Random %', 'Top Precision@10 vs Random %']].max()) + 2)\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "plot_topk_vs_randomk(df_results_final)\n"
      ],
      "metadata": {
        "id": "-gQStzbStFB8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Surrogate Fidelity: Measuring Explainability\n",
        "\n",
        "This code evaluates whether the technique used (RankingSHAP + Decision Tree surrogate) allows us to **explain the LambdaMART model's predictions**.\n",
        "\n",
        "### How it works:\n",
        "- We train a **LambdaMART model (LightGBM Ranker)** on the full feature set.\n",
        "- We select the **top-12 features** using **RankingSHAP** attributions (mean over queries).\n",
        "- We fit a **DecisionTreeRegressor (max_depth=3)** on the top-12 features, aiming to approximate the LambdaMART predictions.\n",
        "- We compute:\n",
        "  - **RÂ² score:** How well the surrogate's predictions match the LambdaMART predictions (closer to 1 means higher fidelity).\n",
        "  - **MSE:** Mean squared error between surrogate predictions and LambdaMART predictions (lower is better)."
      ],
      "metadata": {
        "id": "DOVnMqnKcmzX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "import lightgbm as lgb\n",
        "\n",
        "# === This script trains a LambdaMART model (LightGBM ranker) on MQ2008 LETOR data,\n",
        "# uses RankingSHAP attributions to select the top-k features,\n",
        "# fits a surrogate DecisionTreeRegressor to mimic the ranker's predictions using only the top-k features,\n",
        "# and evaluates the fidelity (RÂ² and MSE) of the surrogate model. ===\n",
        "\n",
        "# === Function to read a LETOR file ===\n",
        "def read_letor_file(filepath):\n",
        "    X, y, qids = [], [], []\n",
        "    with open(filepath, \"r\") as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split()\n",
        "            if len(parts) < 2:\n",
        "                continue\n",
        "            y_val = int(parts[0])\n",
        "            qid = int(parts[1].split(\":\")[1])\n",
        "            features = []\n",
        "            for item in parts[2:]:\n",
        "                if item.startswith(\"#\"):\n",
        "                    break\n",
        "                _, val = item.split(\":\")\n",
        "                features.append(float(val))\n",
        "            X.append(features)\n",
        "            y.append(y_val)\n",
        "            qids.append(qid)\n",
        "    return np.array(X), np.array(y), np.array(qids)\n",
        "\n",
        "# === Load data ===\n",
        "train_X, train_y, train_qids = read_letor_file(\"data/MQ2008/Fold1/train.txt\")\n",
        "test_X, test_y, test_qids = read_letor_file(\"data/MQ2008/Fold1/test_1.txt\")\n",
        "\n",
        "# === Compute group sizes correctly for LightGBM ===\n",
        "df_train_qid = pd.DataFrame({'qid': train_qids})\n",
        "group_sizes = df_train_qid.groupby('qid').size().tolist()\n",
        "\n",
        "# === Load RankingSHAP feature attributions and compute top-k features ===\n",
        "df_shap = pd.read_csv(\"results/results_MQ2008/feature_attributes/rankingshap.csv\")\n",
        "query_cols = [col for col in df_shap.columns if col != \"feature_number\"]\n",
        "df_shap['mean_attribution'] = df_shap[query_cols].mean(axis=1)\n",
        "df_shap_sorted = df_shap.sort_values(by='mean_attribution', ascending=False).reset_index(drop=True)\n",
        "sorted_feature_indices = [int(f) - 1 for f in df_shap_sorted['feature_number']]  # zero-indexed\n",
        "\n",
        "# Select top-k\n",
        "k = 12\n",
        "top_k_indices = sorted_feature_indices[:k]\n",
        "\n",
        "# === Train LambdaMART model ===\n",
        "lgbm_model = lgb.LGBMRanker(objective='lambdarank', random_state=42)\n",
        "lgbm_model.fit(train_X, train_y, group=group_sizes)\n",
        "\n",
        "# === Get predictions from the original model ===\n",
        "y_pred_original = lgbm_model.predict(test_X)\n",
        "\n",
        "# === Prepare data for the surrogate model (using only top-k features) ===\n",
        "test_X_topk = test_X[:, top_k_indices]\n",
        "\n",
        "# === Train surrogate model (Decision Tree Regressor) ===\n",
        "surrogate = DecisionTreeRegressor(max_depth=3, random_state=42)\n",
        "surrogate.fit(test_X_topk, y_pred_original)\n",
        "\n",
        "# === Evaluate fidelity of the surrogate model ===\n",
        "y_pred_surrogate = surrogate.predict(test_X_topk)\n",
        "r2 = r2_score(y_pred_original, y_pred_surrogate)\n",
        "mse = mean_squared_error(y_pred_original, y_pred_surrogate)\n",
        "\n",
        "print(f\"\\n=== Surrogate Fidelity ===\")\n",
        "print(f\"RÂ²: {r2:.4f}\")\n",
        "print(f\"MSE: {mse:.6f}\")\n"
      ],
      "metadata": {
        "id": "vvGuijmwcCH8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}